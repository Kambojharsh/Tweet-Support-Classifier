{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kambojharsh/Tweet-Support-Classifier/blob/main/BTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPjJgNOILtYA"
      },
      "source": [
        "# Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELCWcZzlEgos",
        "outputId": "76a8f0fe-ef26-4500-f298-c5fc8fcc7151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ3j9Hu4Pp5v",
        "outputId": "4bd39826-23f9-4498-9e7b-a7fff4bdccee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/421.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m327.7/421.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJz7xUkunNEP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "corpus=open(\"/content/drive/MyDrive/Deep Learning/words.txt\").read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJIHCg_L5Ni"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random as rd\n",
        "import re\n",
        "import emoji\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df= pd.read_csv('/content/drive/MyDrive/Deep Learning/annotations - Final_Dataset.csv')\n",
        "df = df[df['Reply_Class'] != 'Lang']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF-2sQVjcpxg"
      },
      "outputs": [],
      "source": [
        "hashtag_cache = {}\n",
        "\n",
        "def separate_hashtag_words(sentence):\n",
        "    # Check if the result is cached\n",
        "    sentence = sentence.lower()\n",
        "    if sentence in hashtag_cache:\n",
        "        return hashtag_cache[sentence]\n",
        "\n",
        "    # Calculate and cache the result\n",
        "    hashtags = re.findall(r'#(\\w+)', sentence.lower())\n",
        "    store = []\n",
        "    for hashtag in hashtags:\n",
        "        list_words = []\n",
        "        words = re.split(r'(\\d+)', hashtag)\n",
        "        def func():\n",
        "            for word in words:\n",
        "                list_words.append(word)\n",
        "            return list_words\n",
        "        word_list = func()\n",
        "        for element in word_list:\n",
        "            if element.isdigit():\n",
        "                store.append(element)\n",
        "            else:\n",
        "                new_word = element\n",
        "                word_separated, val1, val2 = '', 0, 0\n",
        "                while val2 < 30:\n",
        "                    for each_word in range(len(new_word), 0, -1):\n",
        "                        if new_word[val1:each_word] in corpus:\n",
        "                            word_separated = word_separated + ' ' + new_word[val1:each_word]\n",
        "                            val1 = each_word\n",
        "                            break\n",
        "                    val2 += 1\n",
        "                store.append(word_separated.strip())\n",
        "\n",
        "    # Format the output string\n",
        "    output_string = ' '.join(str(x) for x in store).title()\n",
        "    if output_string.endswith(\",\"):\n",
        "        output_string = output_string[:-1]  # Remove the trailing comma\n",
        "    # Cache the result\n",
        "    hashtag_cache[sentence] = output_string\n",
        "    return output_string.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWm3X9naS57z",
        "outputId": "40df2ff5-c326-46bc-e865-4f570c965710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sanctions on pakistan\n"
          ]
        }
      ],
      "source": [
        "\n",
        "output_string = separate_hashtag_words(\"#sanctionsonpakistan\")\n",
        "print(output_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glQKCgfDN0Wb"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIHYQsj-N55i",
        "outputId": "6edd2f18-b079-4e2e-cd0c-900d9b92b38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kissing face with smiling eyes kissing face with closed eyes here is an example tweet user example pms us usa sanctionpakistan face with tears of joy skull skull do a lot\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# def pre_process_tweet(tweet):\n",
        "#     # Remove \\n from the end of the sentence\n",
        "#     tweet = tweet.strip('\\n')\n",
        "\n",
        "#     # Use regex to remove '@' only if followed by a word\n",
        "#     tweet = re.sub(r'@(\\w+)', r'\\1', tweet)\n",
        "\n",
        "#     # Remove any URL\n",
        "#     tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "#     tweet = re.sub(r\"www\\S+\", \"\", tweet)\n",
        "\n",
        "#     # Convert emojis to their underscore-separated representation\n",
        "#     tweet = emoji.demojize(tweet)\n",
        "#     tweet = re.sub(r':(\\w+):', r'\\1 ', tweet)\n",
        "\n",
        "#     # Convert underscore-separated words to space-separated words\n",
        "#     tweet = re.sub(r'_', ' ', tweet)\n",
        "\n",
        "#     # Convert dash-separated words to space-separated words\n",
        "#     tweet = re.sub(r'-', ' ', tweet)\n",
        "\n",
        "#     # Replace &amp with 'and'\n",
        "#     tweet = tweet.replace('&amp','and')\n",
        "#     tweet = tweet.replace('&AMP','and')\n",
        "#     tweet = tweet.replace('â','')\n",
        "\n",
        "#     # Replace U.S. with 'usa'\n",
        "#     tweet = tweet.replace('U.S.', 'usa')\n",
        "#     tweet = tweet.replace('US','usa')\n",
        "\n",
        "#     # Remove colons from the end of the sentences (if any)\n",
        "#     tweet = tweet.strip()\n",
        "#     if tweet.endswith(':'):\n",
        "#         tweet = tweet[:-1]\n",
        "\n",
        "#     # Remove hash-tags symbols\n",
        "#     tweet = tweet.replace('#', '')\n",
        "\n",
        "#     # Convert every word to lowercase\n",
        "#     tweet = tweet.lower()\n",
        "\n",
        "#     # Remove punctuations\n",
        "#     tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#     # Trim extra spaces\n",
        "#     tweet = \" \".join(tweet.split())\n",
        "\n",
        "#     return tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# tweet = \"😙😚Here is an 'example' tweet! @user #example PM's us US #sanctionpakistan http://example.com  😂💀💀do a lot !!\"\n",
        "# processed_tweet = pre_process_tweet(tweet)\n",
        "# print(processed_tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pre_process_tweet(tweet):\n",
        "    # Remove \\n from the end of the sentence\n",
        "    tweet = tweet.strip('\\n')\n",
        "\n",
        "    # Convert emojis to their descriptions\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    # Add spaces between emoji descriptions\n",
        "    tweet = re.sub(r'(:\\w+:)', lambda x: x.group(0).replace(':', ' '), tweet)\n",
        "\n",
        "    # Remove any URL\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"www\\S+\", \"\", tweet)\n",
        "\n",
        "    # Convert underscore-separated words to space-separated words\n",
        "    tweet = re.sub(r'_', ' ', tweet)\n",
        "\n",
        "    # Convert dash-separated words to space-separated words\n",
        "    tweet = re.sub(r'-', ' ', tweet)\n",
        "\n",
        "    # Replace &amp with 'and'\n",
        "    tweet = tweet.replace('&amp','and')\n",
        "    tweet = tweet.replace('&AMP','and')\n",
        "\n",
        "    # Replace U.S. with 'usa'\n",
        "    tweet = tweet.replace('U.S.', 'usa')\n",
        "    tweet = tweet.replace('US','usa')\n",
        "    # Remove colons from the end of the sentences (if any)\n",
        "    tweet = tweet.strip()\n",
        "    if tweet.endswith(':'):\n",
        "        tweet = tweet[:-1]\n",
        "\n",
        "    # Split tweet into words\n",
        "    # words = tweet.split()\n",
        "\n",
        "    # # Iterate over words and replace hashtag words\n",
        "    # for i, word in enumerate(words):\n",
        "    #     if word.startswith('#'):\n",
        "    #         words[i] = separate_hashtag_words(word)\n",
        "\n",
        "    # tweet = ' '.join(words)\n",
        "    # tweet = re.sub(r'#\\w+', lambda x: separate_hashtag_words(x.group()), tweet)\n",
        "    # Remove hash-tags symbols and add spaces between words\n",
        "    # tweet = re.sub(r'#(\\w+)', r' \\1', tweet)\n",
        "\n",
        "    # Convert every word to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    tweet = tweet.replace('#sanctionpakistan','sanction pakistan ')\n",
        "    tweet = tweet.replace('#unitednations','united nations ')\n",
        "    tweet = tweet.replace('#endproxywar','end proxy war ')\n",
        "    tweet = tweet.replace('#saveafghanistan','save afghanistan ')\n",
        "    tweet = tweet.replace('#blamegameonpakistan','blame game on pakistan ')\n",
        "    tweet = tweet.replace('#afghanlivesmatter','afghan lives matter ')\n",
        "    tweet = tweet.replace('#pakistaniwarcrimes','pakistani war crimes ')\n",
        "    tweet = tweet.replace('#endproxywarinafghanistan','end proxy war in afghanistan ')\n",
        "\n",
        "    tweet = tweet.replace('#', '')\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Trim extra spaces\n",
        "    tweet = \" \".join(tweet.split())\n",
        "\n",
        "    return tweet\n",
        "\n",
        "# Example usage:\n",
        "tweet = \"#sanctionpakistan\"\n",
        "processed_tweet = pre_process_tweet(tweet)\n",
        "print(processed_tweet)\n",
        "tweet = \"😙😚Here is an 'example' tweet! @user #example PM's us US  #sanctionpakistan http://example.com  😂💀💀do a lot !!\"\n",
        "processed_tweet = pre_process_tweet(tweet)\n",
        "print(processed_tweet)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSNgWjo5qQWn",
        "outputId": "4119ffef-bdab-4fd3-d8fa-24229500537b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanction pakistan\n",
            "kissing face with smiling eyes kissing face with closed eyes here is an example tweet user example pms us usa sanction pakistan face with tears of joy skull skull do a lot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTO-gr9ZD1qW"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].apply(pre_process_tweet)\n",
        "df['Reply'] = df['Reply'].apply(pre_process_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOPa-rPUXuO2",
        "outputId": "a4918289-d408-4457-9291-880110d27ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanction pakistan\n"
          ]
        }
      ],
      "source": [
        "print(df['Reply'][30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1xxFtsvD8rn"
      },
      "source": [
        "# Tokenization and Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UAyJejiEICN",
        "outputId": "f50ecd7c-d0e1-4414-da0a-62d7e4dcc050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reply tokens: ['sanction', 'pakistan', 'united', 'nations', 'united', 'nations', 'high', 'commissioner', 'for', 'human', 'rights', 'united', 'nations', 'general', 'assemblyinternational', 'court', 'of', 'justice', 'international', 'courts', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[  57   18  361  760  361  760 1302 4357   11  518  599  361  760  855\n",
            " 4358 2408    4  500  479 3029    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Text'] + df['Reply'])\n",
        "\n",
        "max_sequence_length = 60\n",
        "\n",
        "sequences_text = tokenizer.texts_to_sequences(df['Text'])\n",
        "sequences_reply = tokenizer.texts_to_sequences(df['Reply'])\n",
        "\n",
        "padded_sequences_text = pad_sequences(sequences_text, maxlen=max_sequence_length, padding='post')\n",
        "padded_sequences_reply = pad_sequences(sequences_reply, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "reply_tokens = [tokenizer.index_word.get(token, '') for token in padded_sequences_reply[11]]\n",
        "print(\"Reply tokens:\", reply_tokens)\n",
        "print(padded_sequences_reply[11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTeO-iUEKM5"
      },
      "source": [
        "# Glove dictionary creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKYZ3S4pEO5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = dict()\n",
        "\n",
        "def add_to_dict(d, filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.split(' ')\n",
        "\n",
        "      try:\n",
        "        d[line[0]] = np.array(line[1:], dtype=float)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "add_to_dict(words, '/content/drive/MyDrive/Deep Learning/glove.twitter.27B.50d.txt')\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiSwk_97ESin"
      },
      "source": [
        "#Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bkDrxnQHi9A",
        "outputId": "3733d198-d882-4e16-9abe-9e109e20e263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.88084   0.30409   0.52879  -1.0601   -0.61752   0.36665  -1.0673\n",
            "  0.58799   0.30702  -0.39185  -0.1807   -2.1897   -2.9295    0.085642\n",
            "  0.46994  -0.66581  -0.81312  -0.28203   0.59902   0.063795 -0.49994\n",
            " -0.015774 -0.44039   0.38011  -0.064951  0.60016  -0.41667  -0.20307\n",
            "  0.14874   0.6782    1.0253   -0.51524   0.11897   0.3691    1.8187\n",
            " -0.35305  -0.22244   0.77367   0.16845  -1.8854   -0.09891  -0.39681\n",
            "  0.064795  0.23272  -0.16004   0.33406   0.012795 -0.84146   0.71641\n",
            "  0.40234 ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Function to get embeddings for sequences\n",
        "def get_embeddings(sequences, word_dict):\n",
        "    embedding_dim = 50\n",
        "    # embedding_dim = len(next(iter(word_dict.values())))  # Dimension of GloVe embeddings\n",
        "    num_words = len(tokenizer.word_index) + 1  # Adding 1 for padding token (index 0)\n",
        "\n",
        "    # Initialize embeddings matrix\n",
        "    embeddings_matrix = np.zeros((len(sequences), max_sequence_length, embedding_dim))\n",
        "\n",
        "    # Iterate over sequences\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # Iterate over tokens in each sequence\n",
        "        for j, token_index in enumerate(seq):\n",
        "            if token_index != 0:  # Skip padding token\n",
        "                word = tokenizer.index_word.get(token_index)\n",
        "                if word in word_dict:\n",
        "                    embeddings_matrix[i, j, :] = word_dict[word]\n",
        "                else:\n",
        "                    # If word not in GloVe dictionary, use zero vector\n",
        "                    embeddings_matrix[i, j, :] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embeddings_matrix\n",
        "\n",
        "# Get embeddings for padded_sequences_text and padded_sequences_reply\n",
        "embeddings_text = get_embeddings(padded_sequences_text, words)\n",
        "embeddings_reply = get_embeddings(padded_sequences_reply, words)\n",
        "\n",
        "# Print the shape of embeddings for verification\n",
        "# print(\"Shape of embeddings_text:\", embeddings_text.shape)\n",
        "# print(\"Shape of embeddings_reply:\", embeddings_reply.shape)\n",
        "\n",
        "print(embeddings_reply[30][1]) #1st word of 1st tweet text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mFxr3g6ABY_"
      },
      "source": [
        "# CREATION OF TRAIN, TEST, VAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaeMmuTAD44I"
      },
      "outputs": [],
      "source": [
        "y=df['Reply_Class']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxz6YuygAI9B",
        "outputId": "77d06bc4-8468-4bef-e13d-2acb7ca6a4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_text shape: (2291, 60, 50)\n",
            "X_val_text shape: (286, 60, 50)\n",
            "X_test_text shape: (287, 60, 50)\n",
            "X_train_reply shape: (2291, 60, 50)\n",
            "X_val_reply shape: (286, 60, 50)\n",
            "X_test_reply shape: (287, 60, 50)\n",
            "y_train shape: (2291,)\n",
            "y_val shape: (286,)\n",
            "y_test shape: (287,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your data stored in embeddings_text and embeddings_reply arrays\n",
        "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
        "X_train_text, X_temp_text = train_test_split(embeddings_text, test_size=0.2, random_state=42)\n",
        "X_val_text, X_test_text = train_test_split(X_temp_text, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train_reply, X_temp_reply = train_test_split(embeddings_reply, test_size=0.2, random_state=42)\n",
        "X_val_reply, X_test_reply = train_test_split(X_temp_reply, test_size=0.5, random_state=42)\n",
        "\n",
        "# Assuming you have labels stored in a variable y\n",
        "# Split the labels accordingly\n",
        "y_train, y_temp = train_test_split(y, test_size=0.2, random_state=42)\n",
        "y_val, y_test = train_test_split(y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert the arrays to numpy arrays\n",
        "X_train_text = np.array(X_train_text)\n",
        "X_val_text = np.array(X_val_text)\n",
        "X_test_text = np.array(X_test_text)\n",
        "\n",
        "X_train_reply = np.array(X_train_reply)\n",
        "X_val_reply = np.array(X_val_reply)\n",
        "X_test_reply = np.array(X_test_reply)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"X_train_text shape:\", X_train_text.shape)\n",
        "print(\"X_val_text shape:\", X_val_text.shape)\n",
        "print(\"X_test_text shape:\", X_test_text.shape)\n",
        "print(\"X_train_reply shape:\", X_train_reply.shape)\n",
        "print(\"X_val_reply shape:\", X_val_reply.shape)\n",
        "print(\"X_test_reply shape:\", X_test_reply.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential([])\n",
        "\n",
        "model.add(layers.Input(shape=(60, 50)))\n",
        "model.add(layers.LSTM(64, return_sequences=True))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.LSTM(64, return_sequences=True))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.LSTM(64, return_sequences=True))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "wNXiPpgQ89Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICo-1kKs9NCF",
        "outputId": "73803bfa-8da3-483f-eee0-a04f9809a91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 60, 64)            29440     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 60, 64)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 60, 64)            33024     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 60, 64)            0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 60, 64)            33024     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 60, 64)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3840)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3841      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 99329 (388.00 KB)\n",
            "Trainable params: 99329 (388.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouM0ftSF_95Q"
      },
      "source": [
        "#LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvCBfCSB7vPP",
        "outputId": "59e39a0d-2a00-4470-ac35-b217508fc97b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 64)                29440     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29440 (115.00 KB)\n",
            "Trainable params: 29440 (115.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "1/1 [==============================] - 0s 401ms/step\n",
            "Shape of intermediate output: (1, 64)\n",
            "Intermediate output:\n",
            "[[ 6.86205531e-05 -6.04103889e-06  8.04972879e-05 -2.70733854e-05\n",
            "   5.62488785e-05 -4.27173181e-05 -1.26177227e-04 -4.11624678e-05\n",
            "  -7.58803726e-05  1.77580168e-05  4.34321373e-05  2.31117247e-05\n",
            "  -9.52698683e-05 -6.64584804e-05  2.39157962e-05 -8.82857858e-05\n",
            "   8.96582278e-05  2.58536766e-05 -3.29308568e-05  1.40438806e-05\n",
            "  -7.84855365e-05 -5.29215395e-05 -1.11455192e-05  4.62953685e-05\n",
            "  -8.30017743e-05 -1.96008637e-04 -1.71533873e-04  5.34708124e-05\n",
            "   1.17577467e-04 -1.01700927e-04  5.82420253e-05  5.35568834e-05\n",
            "   1.80151692e-05 -1.27532927e-04  6.62004531e-05  8.21997528e-05\n",
            "  -6.43000094e-05  6.30335053e-05 -1.72822365e-05 -1.33028647e-04\n",
            "   8.73810568e-05  8.27658514e-05 -2.72214911e-05 -6.68880512e-06\n",
            "   2.44542639e-06 -5.91495809e-05 -9.87368039e-05  6.58212230e-05\n",
            "   1.80805382e-05  1.93764528e-04  4.55504123e-05  6.60061123e-05\n",
            "  -1.09994711e-04 -2.75619350e-05  3.44241598e-05 -8.85055633e-05\n",
            "  -3.93848459e-05 -1.11239504e-04 -3.30503610e-07 -2.90395965e-05\n",
            "  -1.33269083e-07 -1.60826268e-04  7.25786449e-05  2.67043451e-05]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# Define the input embedding (example embedding of a single sentence)\n",
        "embedding = embeddings_reply[0] # Example embedding of shape (60, 50)\n",
        "\n",
        "# Define the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the LSTM layer to the model\n",
        "model.add(LSTM(64, input_shape=(60, 50)))\n",
        "\n",
        "# Compile the model (not necessary for this demonstration)\n",
        "model.compile(optimizer='adam', loss='mse')  # Any dummy optimizer and loss for demonstration purpose\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Predict the intermediate output for the input embedding\n",
        "intermediate_output = model.predict(np.expand_dims(embedding, axis=0))\n",
        "\n",
        "# Print the shape of the intermediate output\n",
        "print(\"Shape of intermediate output:\", intermediate_output.shape)\n",
        "\n",
        "# Print the intermediate output itself\n",
        "print(\"Intermediate output:\")\n",
        "print(intermediate_output)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}